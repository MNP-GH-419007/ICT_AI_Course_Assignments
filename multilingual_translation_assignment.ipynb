{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuuaYqEt_TXg",
    "outputId": "c2be574e-80bf-40ea-a53b-3d388ff5d8cc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/incoder-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Python code input\n",
    "source_code = '''\n",
    "# Translate this Python function to Java\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "# Java version:\n",
    "'''\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(source_code, return_tensors=\"pt\")\n",
    "\n",
    "# Remove token_type_ids if present\n",
    "inputs = {k: v for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=256)\n",
    "\n",
    "# Decode and print\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Translated Code:\\n\")\n",
    "print(result)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dd64011a40df46fc89797c69a51db96c",
      "2fff79ef017e4985897c3c4a945f8817",
      "31f956f86345489ab8605eb058c88450",
      "6f4f8e8a4cc445eb8530c672efb48c1c",
      "c453686acecc4f9cbfcb49cb704dd514",
      "5eba1bdd71744f79b8b7adc1e292bf61",
      "3eb1ebe0d0e54278be2419c87e221a0d",
      "ffeb717704bb48ec9b61cb70644c7e3f",
      "47cea374892b4098ac7aa726a61a9c81",
      "9c690e7936b042558e334b1e5bc880cd",
      "e587d4f5a7684c30b35b25494a9aeaf9",
      "9abc4742e17c446d803314b05372e830",
      "f4106435f0424216bb9fc0681bea0ac9",
      "b5f8bcb3031042a99a37f69646362f5c",
      "340ab723abc9445f80d942110bce8e9d",
      "9ce50bbdc9d94c61b90abf0efcd5faab",
      "e8f30c5db7e84d4a9d556d4abd45abdc",
      "494929f002134cce82025e198f6168fb",
      "57dd10c1f98247a2b0f2b3e7e537ab4f",
      "9932d21a05cd44559b7f3289c074d15e",
      "e3a8a7aa6826423e8ecc0a6cf5770f30",
      "51df3823c22b47a19ed17694b2f9c3a1",
      "2a9edc32e24a4faf978d7dc716bac3cc",
      "172dd88dd21f4b9da4999c12b68b1f83",
      "da0f097922a44de2a900808520ae7308",
      "faa57563dd954c798b86f4dfe95be703",
      "1d3643b973a344ebbbaf6f0c73d836fd",
      "bfa5bcb196a44211be515f256ee724e5",
      "73908a0e001343c5851e6eb6981714d9",
      "021062729a584f4c94995a6c90f3510f",
      "f1d86977f29f490aba484d88abb09a8a",
      "f8c733e679bc4aaaa6cb9cecde3082a5",
      "5bb4aefc20c2430db25dfec782dcba6a",
      "7225de27b8ce43e394ac79c0bb897f5e",
      "730b53026b5d41c490df499fb60b41b1",
      "79832a1df6fa4ea0a7eb5b5c0f0e9170",
      "24558cc1e1ed41bf835f156d8effb695",
      "c51bde95fbd44a7ea51e20f664fc1941",
      "afb18ed3bf504b1db315b865d32c7aba",
      "5f143df432e84389aca7a6110f557894",
      "48b9ee351b624784bc6878a29cb24756",
      "1227e863b68541d48b38754e56146a14",
      "e29d5cf085b344d18d33777041f84f80",
      "2fca56fb26774239be3aee09361bf2d5",
      "28e993e823954b459165918ca9186865",
      "19f622f997c44165b9248cbd2ea4bdbe",
      "9117f5b58f2949a8a8827fa238b706e4",
      "92ae8fcb315e4f718ae4ce1641bdc649",
      "d137a6876aa24cd689495646603b381e",
      "6bdb7ff1bf6349a78505bd7fe4f98161",
      "50879ff7dd8242e68e6fc1817dbd26a9",
      "57e534584c3d4d8caf0c0247f5318c35",
      "6b4e66157d5544fa9d70408e1ac34f50",
      "0498e0a27a2f45c383077b601930aaa7",
      "0c016323155f4929860be9691cf77ca3",
      "6b202ab614344cbdb5beb53926a92f83",
      "698439bbec9f4f449c8c59ab5699d771",
      "d0b0f25756d34705affb66265fabbb41",
      "d2c3859824cc48cba250a80e4cd55ed7",
      "0359feb63dd14a75927da418024aa20c",
      "43fe7535f36c4b2ebc8b79ec796067ac",
      "4d2301cd3af94800bf7512d8d95cb8a9",
      "200cb7090bd74793886abf68e9cf69b3",
      "65c327cb25c6407c85732ec76a791a6a",
      "d3f200f3f8564ea28fc1106376cbee1f",
      "1935887db6934852adf0a8d017e696f3",
      "a33fcf44f41543d0adfa4618475c6eb8",
      "71a51a521a3e4c8ebaea7829d82ead8b",
      "0ddfaa67137c4b71888e7b6bc44d3b0b",
      "6c6d2ff085bc45c6bbda0d78949166e8",
      "8e08a34e4e1e4bfcbd3d12f942ef1fc0",
      "ee6fc9db54a345e9abedb894bb4ef351",
      "206fc948246f4bd9822858677515e40d",
      "7cb8c48662b34563a7ce4d4f83eda06c",
      "cd6016ed70f84354b3e6493edca47923",
      "594bec9603ff4fb99352f0a43fced835",
      "c0473af815b94dbe997342fa9ffc75a0"
     ]
    },
    "id": "Mlm9pqN3_ojB",
    "outputId": "e5cf958f-1bb0-4306-a12a-d647e5365fc5"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd64011a40df46fc89797c69a51db96c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9abc4742e17c446d803314b05372e830"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a9edc32e24a4faf978d7dc716bac3cc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7225de27b8ce43e394ac79c0bb897f5e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28e993e823954b459165918ca9186865"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b202ab614344cbdb5beb53926a92f83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a33fcf44f41543d0adfa4618475c6eb8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translated Code:\n",
      "\n",
      "\n",
      "# Translate this Python function to Java\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "# Java version:\n",
      "# public int add(int a, int b) {\n",
      "#     return a + b;\n",
      "# }\n",
      "</cell>\n",
      "<cell>\n",
      "# Test your function with some random numbers\n",
      "# Make sure you use the same seed for both calls to random()\n",
      "random.seed(42)\n",
      "print(add(3, 4))\n",
      "print(add(3, 4))\n",
      "</cell>\n",
      "<text>\n",
      "Now let's use NumPy to implement the addition operation in pure Python:\n",
      "\n",
      "\n",
      "</text>\n",
      "<cell>\n",
      "# Python version\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "# NumPy version\n",
      "import numpy as np\n",
      "a = np.array([3, 5])\n",
      "b = np.array([4, 6])\n",
      "print(add(a, b))\n",
      "</cell>\n",
      "<text>\n",
      "As you might expect, NumPy takes care of adding the two arrays together, returning us the array:\n",
      "\n",
      "\n",
      "</text>\n",
      "<cell>\n",
      "print(add(a, b))\n",
      "</cell>\n",
      "<text>\n",
      "Of course, if you try to use this operation on two ndarrays containing numbers, NumPy will apply the operation element-wise. So if you have a one-line operation like this:\n",
      "\n",
      "\n",
      "</text>\n",
      "<cell>\n",
      "a = np.array([3, 5])\n",
      "b = np.array([4, 6])\n",
      "print(a + b)\n",
      "</cell>\n",
      "<text>\n",
      "The operation is actually applied on each of the two ndarrays in parallel\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/incoder-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# One-line input for target language\n",
    "target_language = input(\"Enter target language (e.g., Java, C++, JavaScript): \")\n",
    "\n",
    "# Python code input\n",
    "python_function = '''\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "'''\n",
    "\n",
    "# Create prompt\n",
    "source_code = f'''\n",
    "# Translate this Python function to {target_language}\n",
    "{python_function}\n",
    "# {target_language} version:\n",
    "'''\n",
    "\n",
    "# Tokenize and clean inputs\n",
    "inputs = tokenizer(source_code, return_tensors=\"pt\")\n",
    "inputs = {k: v for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=256, num_return_sequences=1)\n",
    "\n",
    "# Decode and print\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\n\u2705 Translated Code to {target_language}:\\n\")\n",
    "print(result)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "245x3Ztt_ona",
    "outputId": "1567ae2f-a98b-42d2-b810-1c93bccd2333"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enter target language (e.g., Java, C++, JavaScript): C++\n",
      "\n",
      "\u2705 Translated Code to C++:\n",
      "\n",
      "\n",
      "# Translate this Python function to C++\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n - 1)\n",
      "\n",
      "# C++ version:\n",
      "# \n",
      "# int factorial(int n) {\n",
      "#     if (n == 0)\n",
      "#         return 1;\n",
      "#     else\n",
      "#         return n * factorial(n - 1);\n",
      "# }\n",
      "\n",
      "# Test your function with some test cases:\n",
      "# \n",
      "# factorial(0) should return 1\n",
      "# factorial(1) should return 1\n",
      "# factorial(2) should return 6\n",
      "# factorial(3) should return 120\n",
      "# factorial(4) should return 720\n",
      "# factorial(5) should return 5040\n",
      "# factorial(6) should return 40320\n",
      "# factorial(7) should return 362880\n",
      "# factorial(8) should return 3628800\n",
      "# factorial(9) should return 39916800\n",
      "# factorial(10) should return 479001600\n",
      "# factorial(11) should return 52428800\n",
      "# factorial(12) should return 576048000\n",
      "# factorial(13) should return 60480000\n",
      "# factorial(14) should return 6000000000\n",
      "# factorial(15) should return 60000000000\n",
      "# factorial(16) should return 600000000000\n",
      "# factorial(17) should return 6000000000000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/incoder-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define the list of target languages\n",
    "target_languages = [\"Java\", \"C++\", \"JavaScript\", \"C#\", \"Go\", \"Rust\"]\n",
    "\n",
    "# Python function to translate\n",
    "python_function = '''\n",
    "def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "'''\n",
    "\n",
    "# Loop over each language and translate\n",
    "for target_language in target_languages:\n",
    "    print(f\"\\n\ud83d\udd01 Translating to {target_language}...\\n\")\n",
    "\n",
    "    # Create prompt\n",
    "    source_code = f'''\n",
    "# Translate this Python function to {target_language}\n",
    "{python_function}\n",
    "# {target_language} version:\n",
    "'''\n",
    "\n",
    "    # Tokenize and clean inputs\n",
    "    inputs = tokenizer(source_code, return_tensors=\"pt\")\n",
    "    inputs = {k: v for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, max_length=256, num_return_sequences=1)\n",
    "\n",
    "    # Decode and display\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\u2705 Translated Code to {target_language}:\\n\")\n",
    "    print(result.split(f\"# {target_language} version:\")[-1].strip())  # Just the translated part\n",
    "    print(\"-\" * 50)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRZk0OK-hpRw",
    "outputId": "f209fd79-b8b0-4060-cbf9-3dc905f718a4"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udd01 Translating to Java...\n",
      "\n",
      "\u2705 Translated Code to Java:\n",
      "\n",
      "# public int factorial(int n) {\n",
      "#     if (n == 0) {\n",
      "#         return 1;\n",
      "#     } else {\n",
      "#         return n * factorial(n - 1);\n",
      "#     }\n",
      "# }\n",
      "</code>\n",
      "<|/ a tags=recursion,java,python,recursion |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain me the difference between these two functions?\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "The first one is a recursive function. The second one is a non-recursive function. The first one will call itself until it reaches the base case. The second one will call itself until it reaches the base case and then return its result. Both are recursive functions. You can read more about recursive functions here: https://en.wikipedia.org/wiki/Recursion_(computer_science)\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain me the difference between these two functions?\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "The first one is a recursive function. The second one is a non-recursive function. The first one will call itself until it reaches the base case. The second one will call itself until it reache\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udd01 Translating to C++...\n",
      "\n",
      "\u2705 Translated Code to C++:\n",
      "\n",
      "# \n",
      "# int factorial(int n) {\n",
      "#     if (n == 0)\n",
      "#         return 1;\n",
      "#     else\n",
      "#         return n * factorial(n - 1);\n",
      "# }\n",
      "\n",
      "# Test your function with some test cases:\n",
      "# \n",
      "# factorial(0) should return 1\n",
      "# factorial(1) should return 1\n",
      "# factorial(2) should return 6\n",
      "# factorial(3) should return 120\n",
      "# factorial(4) should return 720\n",
      "# factorial(5) should return 5040\n",
      "# factorial(6) should return 40320\n",
      "# factorial(7) should return 362880\n",
      "# factorial(8) should return 3628800\n",
      "# factorial(9) should return 39916800\n",
      "# factorial(10) should return 479001600\n",
      "# factorial(11) should return 52428800\n",
      "# factorial(12) should return 576048000\n",
      "# factorial(13) should return 60480000\n",
      "# factorial(14) should return 6000000000\n",
      "# factorial(15) should return 60000000000\n",
      "# factorial(16) should return 600000000000\n",
      "# factorial(17) should return 6000000000000\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udd01 Translating to JavaScript...\n",
      "\n",
      "\u2705 Translated Code to JavaScript:\n",
      "\n",
      "function factorial(n) {\n",
      "    if (n == 0) {\n",
      "        return 1;\n",
      "    } else {\n",
      "        return n * factorial(n - 1);\n",
      "    }\n",
      "}\n",
      "\n",
      "# End of Python code\n",
      "</cell>\n",
      "<text>\n",
      "Exercise 3\n",
      "Write a Python function that takes an integer $n$ and returns the factorial of $n$. For example, factorial(5) should return 120.\n",
      "\n",
      "Hint: Use a for loop to iterate from 1 to $n$ and return the factorial of $i$.\n",
      "\n",
      "\n",
      "</text>\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udd01 Translating to C#...\n",
      "\n",
      "\u2705 Translated Code to C#:\n",
      "\n",
      "public int Factorial(int n)\n",
      "{\n",
      "    if (n == 0)\n",
      "    {\n",
      "        return 1;\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "        return n * Factorial(n - 1);\n",
      "    }\n",
      "}\n",
      "</code>\n",
      "<|/ a tags=c#,recursion,c#-4.0,c#-3.0,c# |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain me the difference between your C# code and my C# code? I mean the difference between your C# code and mine.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "I added a C# version to my answer. Hope it helps.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain me the difference between your C# code and mine? I mean the difference between your C# code and mine.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "I added a C# version to my answer. Hope it helps.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain me the difference between your C# code and mine? I mean the difference between your C#\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udd01 Translating to Go...\n",
      "\n",
      "\u2705 Translated Code to Go:\n",
      "\n",
      "func factorial(n *int) int {\n",
      "    if n == 0 {\n",
      "        return 1\n",
      "    } else {\n",
      "        return n * factorial(n - 1)\n",
      "    }\n",
      "}\n",
      "\n",
      "func main() {\n",
      "    n := 5\n",
      "    fmt.Println(factorial(n))\n",
      "}\n",
      "</code>\n",
      "<|/ a tags=go,recursion |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain why you used `*int` instead of `int` in the Go version?\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Because Go doesn't have `int` type. `int` type is `int` in C and `*int` is `*int` in Go. `int` type is `int` in Go, `*int` is `*int` in C.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Thanks for your explanation. Can you please explain why you used `*int` instead of `int` in the Go version?\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "Because Go doesn't have `int` type. `int` type is `int` in C and `*int` is `*int` in Go\n",
      "--------------------------------------------------\n",
      "\n",
      "\ud83d\udd01 Translating to Rust...\n",
      "\n",
      "\u2705 Translated Code to Rust:\n",
      "\n",
      "#[inline]\n",
      "pub fn factorial(n: i32) -> i32 {\n",
      "    if n == 0 {\n",
      "        return 1\n",
      "    } else {\n",
      "        n * factorial(n - 1)\n",
      "    }\n",
      "}\n",
      "</code>\n",
      "<|/ a tags=rust,python |>\n",
      "<| c |>\n",
      "Thanks for your answer. Can you please explain why you used `&mut self` instead of `self`? I thought `self` was a pointer to `self` in Python.\n",
      "<|/ c |>\n",
      "<| c |>\n",
      "`self` is just a pointer to `self` in Python. `self` can point to any object in Rust. `self` can point to any object in Python. `self` can point to any object in Rust. `self` can point to any object in Python. `self` can point to any object in Rust. `self` can point to any object in Python. `self` can point to any object in Rust. `self` can point to any object in Python. `self` can point to any object in Rust. `self` can point\n",
      "--------------------------------------------------\n"
     ]
    }
   ]
  }
 ]
}